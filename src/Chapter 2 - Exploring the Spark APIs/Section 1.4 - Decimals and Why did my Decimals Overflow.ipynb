{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from datetime import datetime\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local\")\n",
    "    .appName(\"Section 1.4 - Decimals and Why did my Decimals overflow\")\n",
    "    .config(\"spark.some.config.option\", \"some-value\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "def get_csv_schema(*args):\n",
    "    return T.StructType([\n",
    "        T.StructField(*arg)\n",
    "        for arg in args\n",
    "    ])\n",
    "\n",
    "def read_csv(fname, schema):\n",
    "    return spark.read.csv(\n",
    "        path=fname,\n",
    "        header=True,\n",
    "        schema=get_csv_schema(*schema)\n",
    "    )\n",
    "\n",
    "import os\n",
    "\n",
    "data_path = \"/data/pets.csv\"\n",
    "base_path = os.path.dirname(os.getcwd())\n",
    "path = base_path + data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decimals and Why did my Decimals overflow\n",
    "\n",
    "Some cases where you would deal with `Decimal` types are if you are talking about money, height, weight, etc. Working with `Decimal` types may appear at first but there are some nuances that will sneak up behind you. We will go through some ways to avoid these nauances as they are hard to debug.\n",
    "\n",
    "Here are some simple jargon that we will use in the follow examples:\n",
    "* `Integer`: The set of numbers including all the whole numbers and their opposites (the positive whole numbers, the negative whole numbers, and zero). ie. -1,0,1,2, etc.\n",
    "* `Irrational Number`: The set including all numbers that are non- terminating, non- repeating decimals. ie. 2.1, 10.5, etc.\n",
    "* `Precision`: the maximum total number of digits.\n",
    "* `Scale`: the number of digits on the right of dot.\n",
    "\n",
    "Source:\n",
    "* [link](https://www.sparknotes.com/math/prealgebra/integersandrationals/terms/)\n",
    "* [link](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.DecimalType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: Working With `Decimal`s in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 - 20\n",
      "Example 2 - 20.2\n",
      "Example 3 - 20.5\n",
      "Example 4 - 20.199999999999999289457264239899814128875732421875\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 1 - {}\".format(Decimal(20)))\n",
    "print(\"Example 2 - {}\".format(Decimal(\"20.2\")))\n",
    "print(\"Example 3 - {}\".format(Decimal(20.5)))\n",
    "print(\"Example 4 - {}\".format(Decimal(20.2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happened?**\n",
    "\n",
    "Let's breakdown the examples above.\n",
    "\n",
    "Example 1:\n",
    "\n",
    "Here we provided a whole number, so nothing special.\n",
    "\n",
    "Example 2:\n",
    "\n",
    "Here we provided a string representing an irrational number. The `precision` and `scale` were preserved.\n",
    "\n",
    "Example 3:\n",
    "\n",
    "Here we provided an irrational number. The `precision` and `scale` were preserved.\n",
    "\n",
    "Example 4:\n",
    "\n",
    "Here we provided an irrational number, but this isn't what we expected? Well this is because it's impossible to provide an exact representation of `20.2` on the computer! If you want to know more about this you can look up \"IEEE floating point representation\". We will keep this in mind for later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Reading in Decimals in Spark (Incorrectly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+---+-----+------+\n",
      "| id|breed_id|nickname|           birthday|age|color|weight|\n",
      "+---+--------+--------+-------------------+---+-----+------+\n",
      "|  1|       1|    King|2014-11-22 12:30:31|  5|brown|    10|\n",
      "|  2|       3|   Argus|2016-11-22 10:05:10| 10| null|     6|\n",
      "|  3|       1|  Chewie|2016-11-22 10:05:10| 15| null|    12|\n",
      "|  3|       2|   Maple|2018-11-22 10:05:10| 17|white|     3|\n",
      "|  4|       2|    null|2019-01-01 10:05:10| 13| null|    10|\n",
      "+---+--------+--------+-------------------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pets = read_csv(\n",
    "    fname=path,\n",
    "    schema=[\n",
    "        (\"id\", T.LongType(), False),\n",
    "        (\"breed_id\", T.LongType(), True),\n",
    "        (\"nickname\", T.StringType(), True),\n",
    "        (\"birthday\", T.TimestampType(), True),\n",
    "        (\"age\", T.LongType(), True),\n",
    "        (\"color\", T.StringType(), True),\n",
    "        (\"weight\", T.DecimalType(), True),\n",
    "    ]\n",
    ")\n",
    "pets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happened?**\n",
    "\n",
    "What happened to our `scalar` values, they weren't read in? This is because the default arguments to the `T.Decimal()` function are `DecimalType(precision=10, scale=0)`. So in order read in the data correctly we need to override these default arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Reading in Decimals in Spark (Correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+---+-----+------+\n",
      "| id|breed_id|nickname|           birthday|age|color|weight|\n",
      "+---+--------+--------+-------------------+---+-----+------+\n",
      "|  1|       1|    King|2014-11-22 12:30:31|  5|brown| 10.00|\n",
      "|  2|       3|   Argus|2016-11-22 10:05:10| 10| null|  5.50|\n",
      "|  3|       1|  Chewie|2016-11-22 10:05:10| 15| null| 12.00|\n",
      "|  3|       2|   Maple|2018-11-22 10:05:10| 17|white|  3.40|\n",
      "|  4|       2|    null|2019-01-01 10:05:10| 13| null| 10.00|\n",
      "+---+--------+--------+-------------------+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pets = read_csv(\n",
    "    fname=path,\n",
    "    schema=[\n",
    "        (\"id\", T.LongType(), False),\n",
    "        (\"breed_id\", T.LongType(), True),\n",
    "        (\"nickname\", T.StringType(), True),\n",
    "        (\"birthday\", T.TimestampType(), True),\n",
    "        (\"age\", T.LongType(), True),\n",
    "        (\"color\", T.StringType(), True),\n",
    "        (\"weight\", T.DecimalType(10,2), True),\n",
    "    ]\n",
    ")\n",
    "pets.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Reading in Large Decimals in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|data|\n",
      "+----+\n",
      "| 100|\n",
      "|null|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(\n",
    "    data=[\n",
    "        (100,),\n",
    "        (2 ** 63,)\n",
    "    ],\n",
    "    schema=['data']\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happened?**\n",
    "\n",
    "Why is the second value null? The second value overflows the max value of and never makes it to Spark (Scala). \n",
    "\n",
    "If a similar error happens then you will need to check your input data as there might be something wrong there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Setting Values in a DataFrame (Incorrectly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'DecimalType can only support precision up to 38;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-14e51ddcba87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m (\n\u001b[1;32m      2\u001b[0m     \u001b[0mpets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'decimal_column'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDecimal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pyspark/sql/functions.pyc\u001b[0m in \u001b[0;36m_\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'DecimalType can only support precision up to 38;'"
     ]
    }
   ],
   "source": [
    "(\n",
    "    pets\n",
    "    .withColumn('decimal_column', F.lit(Decimal(20.2)))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happened?**\n",
    "\n",
    "Remember our python examples above? Well because the `precision` of the Spark `T.DecimalType` is 38 digits, the value went over the precious of the Spark type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Setting Values in a DataFrame (Correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------+-------------------+---+-----+------+--------------+\n",
      "| id|breed_id|nickname|           birthday|age|color|weight|decimal_column|\n",
      "+---+--------+--------+-------------------+---+-----+------+--------------+\n",
      "|  1|       1|    King|2014-11-22 12:30:31|  5|brown| 10.00|          20.2|\n",
      "|  2|       3|   Argus|2016-11-22 10:05:10| 10| null|  5.50|          20.2|\n",
      "|  3|       1|  Chewie|2016-11-22 10:05:10| 15| null| 12.00|          20.2|\n",
      "|  3|       2|   Maple|2018-11-22 10:05:10| 17|white|  3.40|          20.2|\n",
      "|  4|       2|    null|2019-01-01 10:05:10| 13| null| 10.00|          20.2|\n",
      "+---+--------+--------+-------------------+---+-----+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    pets\n",
    "    .withColumn('decimal_column', F.lit(Decimal(\"20.2\")))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happened?**\n",
    "\n",
    "If we provide the irrational number as a string, this solves the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 4: Performing Arthimetrics with `DecimalType`s (Incorrectly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|       weight_in_kgs|   conversion_to_lbs|\n",
      "+--------------------+--------------------+\n",
      "|113.7900000000000...|2.540000000000000000|\n",
      "|113.7900000000000...|2.540000000000000000|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pets = spark.createDataFrame(\n",
    "    data=[\n",
    "        (Decimal('113.790000000000000000'), Decimal('2.54')),\n",
    "        (Decimal('113.790000000000000000'), Decimal('2.54')),\n",
    "    ],\n",
    "    schema=['weight_in_kgs','conversion_to_lbs']\n",
    ")\n",
    "\n",
    "pets.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------+\n",
      "|       weight_in_kgs|   conversion_to_lbs|weight_in_lbs|\n",
      "+--------------------+--------------------+-------------+\n",
      "|113.7900000000000...|2.540000000000000000|   289.026600|\n",
      "|113.7900000000000...|2.540000000000000000|   289.026600|\n",
      "+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    pets\n",
    "    .withColumn('weight_in_lbs', F.col('weight_in_kgs') * F.col('conversion_to_lbs'))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happened?**\n",
    "\n",
    "This used to overflow... Guess they updated it ðŸ˜…."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 4: Performing Arthimetrics Operations with DecimalTypes (Correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+-------------+\n",
      "|weight_in_kgs|conversion_to_lbs|weight_in_lbs|\n",
      "+-------------+-----------------+-------------+\n",
      "|       113.79|             2.54|     289.0266|\n",
      "|       113.79|             2.54|     289.0266|\n",
      "+-------------+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    pets\n",
    "    .withColumn('weight_in_kgs', F.col('weight_in_kgs').cast('Decimal(20,2)'))\n",
    "    .withColumn('conversion_to_lbs', F.col('conversion_to_lbs').cast('Decimal(20,2)'))\n",
    "    .withColumn('weight_in_lbs', F.col('weight_in_kgs') * F.col('conversion_to_lbs'))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What Happened?**\n",
    "\n",
    "Before doing the calculations, we truncated (with the help of the `cast` function, which we will learn about in the later chapters) all of the values to be only 2 `scalar` digits at most. This should be the way you should be performing your arithmetic operations with `Decimal Types`. Ideally you should know the minimal number of `scalar` digits needed for each datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* We learnt that you should always initial `Decimal` types using string represented numbers, if they are Irrational Number.\n",
    "* When reading in `Decimal` types, you should explicitly override the default arguments of the Spark type and make sure that the underlying data is correct.\n",
    "* When performing arthimetrics operations with decimal types you should always truncate the scalar digits to the lowest number of digits as possible, if you haven't already."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
